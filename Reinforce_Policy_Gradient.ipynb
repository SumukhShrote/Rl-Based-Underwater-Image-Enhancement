{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = cv2.imread('g:\\Images\\Original\\image_1721814223451221704.png')\n",
    "input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
    "plt.imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_features(image):\n",
    "#     # Create ORB detector\n",
    "#     orb = cv2.ORB_create()\n",
    "\n",
    "#     # Detect keypoints using ORB\n",
    "#     keypoints = orb.detect(image, None)\n",
    "\n",
    "#     # Compute descriptors\n",
    "#     keypoints, descriptors = orb.compute(image, keypoints)\n",
    "\n",
    "#     # Draw the detected keypoints on the image\n",
    "#     output_image = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "#     # Count the number of detected features\n",
    "#     num_features = len(keypoints)\n",
    "#     # print(f\"Number of ORB features detected: {num_features}\")\n",
    "\n",
    "#     # Display the image with keypoints\n",
    "#     # plt.imshow(output_image)\n",
    "#     # plt.axis('off')\n",
    "#     # plt.show()\n",
    "    \n",
    "#     return num_features, output_image\n",
    "\n",
    "# features, image = calculate_features(input_img)\n",
    "# print(features)\n",
    "# plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(image):\n",
    "\n",
    "    fast12 = cv2.FastFeatureDetector_create(nonmaxSuppression = True)\n",
    "\n",
    "    keypoints = fast12.detect(image, None)\n",
    "\n",
    "    # Draw the detected keypoints on the image\n",
    "    output_image = cv2.drawKeypoints(image, keypoints, None, color=(255, 0, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "    # Count the number of detected features\n",
    "    num_features = len(keypoints)\n",
    "    # print(f\"Number of FAST-12 features detected: {num_features}\")\n",
    "\n",
    "    # Display the image with keypoints\n",
    "    # plt.imshow(output_image)\n",
    "    return num_features, output_image\n",
    "\n",
    "features, image = calculate_features(input_img)\n",
    "print(features)\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the list of image enhancement algorithms\n",
    "image_enhancement_algorithms = ['WB','C_Up','C_Down','Bs_Up','B_Down','CLAHE']\n",
    "\n",
    "# Define state space (number of features in intervals of hundreds)\n",
    "state_space = ['F0','F1','F2','F3','F4','F5']\n",
    "\n",
    "# # Define the environment (for simplicity, we assume a deterministic environment)\n",
    "# def get_features(image, algorithm):\n",
    "#     # Simulated function to compute the number of features\n",
    "#     if algorithm == \"Algorithm1\":\n",
    "#         return 2 * image  # Adjust this for different algorithms\n",
    "#     elif algorithm == \"Algorithm2\":\n",
    "#         return 3 * image\n",
    "#     elif algorithm == \"Algorithm3\":\n",
    "#         return 4 * image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the REINFORCE policy network\n",
    "class PolicyNetwork(tf.keras.Model):\n",
    "    def __init__(self, num_actions):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.dense1 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.dense2 = tf.keras.layers.Dense(num_actions, activation='softmax')\n",
    "\n",
    "    def call(self, state):\n",
    "        x = self.dense1(state)\n",
    "        return self.dense2(x)\n",
    "\n",
    "# Create the policy network and optimizer\n",
    "policy_net = PolicyNetwork(len(image_enhancement_algorithms))\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLAHE(image):\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "    clahe_image = clahe.apply(gray_image)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        clahe_image = cv2.cvtColor(clahe_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return clahe_image\n",
    "image = CLAHE(input_img)\n",
    "plt.imshow(image)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_balance(image):\n",
    "   \n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    l, a, b = cv2.split(lab_image)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    balanced_lab_image = cv2.merge((cl, a, b))\n",
    "\n",
    "    balanced_image = cv2.cvtColor(balanced_lab_image, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return balanced_image\n",
    "image = white_balance(input_img)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contrast_Up(image):\n",
    "    \n",
    "    contrasted_image = cv2.convertScaleAbs(image, alpha=1.25, beta=0)\n",
    "    return contrasted_image\n",
    "image = Contrast_Up(input_img)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contrast_Down(image):\n",
    "    \n",
    "    contrasted_image = cv2.convertScaleAbs(image, alpha=0.75, beta=0)\n",
    "    return contrasted_image\n",
    "image = Contrast_Down(input_img)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Brightness_Up(image):\n",
    "    \n",
    "    brightened_image = cv2.convertScaleAbs(image, alpha=1.0, beta=15)\n",
    "    return brightened_image\n",
    "image = Brightness_Up(input_img)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Brightness_Down(image):\n",
    "    \n",
    "    darkened_image = cv2.convertScaleAbs(image, alpha=1.0, beta=-100)\n",
    "    return darkened_image\n",
    "image = Brightness_Down(input_img)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_action(ind,img_inp):\n",
    "        if ind == 0:\n",
    "            denoised = white_balance(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 1:\n",
    "            denoised = Contrast_Up(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 2:\n",
    "            denoised = Contrast_Down(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 3:\n",
    "            denoised = Brightness_Up(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 4:\n",
    "            denoised = Brightness_Down(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 5:\n",
    "            denoised = CLAHE(img_inp)\n",
    "            return denoised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_state(image):\n",
    "        num_of_features, _ = calculate_features(image)\n",
    "        if num_of_features < 0:\n",
    "            return 'F0'\n",
    "        elif num_of_features >= 0 and num_of_features < 100 :\n",
    "            return 'F1'\n",
    "        elif num_of_features <=200 and num_of_features > 100:\n",
    "            return 'F2'\n",
    "        elif num_of_features <=300 and num_of_features > 200:\n",
    "            return 'F3'\n",
    "        elif num_of_features <=400 and num_of_features > 300:\n",
    "            return 'F4'\n",
    "        elif  num_of_features > 400:\n",
    "            return 'F5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_state(image ,action):\n",
    "        next_img = perform_action(action,image)\n",
    "        return [next_img,check_state(next_img)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_value(ft1):\n",
    "        i1 = state_space.index(ft1)\n",
    "        return i1*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_reward(img1,img2):\n",
    "        features_1, _ = calculate_features(img1)\n",
    "        features_2, _ = calculate_features(img2)\n",
    "        feature_difference = features_1 - features_2\n",
    "        \n",
    "        if feature_difference <0:\n",
    "            return -5\n",
    "        elif feature_difference == 0:\n",
    "            return -1\n",
    "        elif feature_difference <= 100 and feature_difference > 0:\n",
    "            return 1\n",
    "        elif feature_difference <= 200 and feature_difference > 100:\n",
    "            return 2\n",
    "        elif feature_difference <= 300 and feature_difference > 200:\n",
    "            return 3\n",
    "        elif feature_difference <= 400 and feature_difference > 300:\n",
    "            return 4\n",
    "        elif feature_difference > 400 :\n",
    "            return 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training loop\n",
    "def train_REINFORCE(num_episodes, discount_factor,input_img):\n",
    "    cumulative_reward = 0\n",
    "    \n",
    "    for episode in range(num_episodes):\n",
    "        state = np.random.choice(state_space)  # Random initial state\n",
    "        state_tensor = tf.constant([[get_feature_value(state)]], dtype=tf.float32)\n",
    "\n",
    "        curr_image = input_img\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            action_probs = policy_net(state_tensor)\n",
    "            action_distribution = tfp.distributions.Categorical(probs=action_probs)\n",
    "            action = action_distribution.sample()\n",
    "            action = int(action.numpy())\n",
    "\n",
    "            if action == 0:\n",
    "                nxt_state =  next_state(curr_image,0)\n",
    "                next_state_tensor = tf.constant([[get_feature_value(nxt_state[1])]], dtype=tf.float32)\n",
    "                den_img = nxt_state[0]\n",
    "                reward = update_reward(den_img,curr_image)\n",
    "                cumulative_reward += reward\n",
    "                    \n",
    "            elif action == 1:\n",
    "                nxt_state =  next_state(curr_image,1)\n",
    "                next_state_tensor = tf.constant([[get_feature_value(nxt_state[1])]], dtype=tf.float32)\n",
    "                den_img = nxt_state[0]\n",
    "                reward = update_reward(den_img,curr_image)\n",
    "                cumulative_reward += reward\n",
    "                    \n",
    "            elif action == 2:\n",
    "                nxt_state =  next_state(curr_image,2)\n",
    "                next_state_tensor = tf.constant([[get_feature_value(nxt_state[1])]], dtype=tf.float32)\n",
    "                den_img = nxt_state[0]\n",
    "                reward = update_reward(den_img,curr_image)\n",
    "                cumulative_reward += reward\n",
    "                    \n",
    "            elif action == 3:\n",
    "                nxt_state =  next_state(curr_image,3)\n",
    "                next_state_tensor = tf.constant([[get_feature_value(nxt_state[1])]], dtype=tf.float32)\n",
    "                den_img = nxt_state[0]\n",
    "                reward = update_reward(den_img,curr_image)\n",
    "                cumulative_reward += reward\n",
    "                    \n",
    "            elif action == 4:\n",
    "                nxt_state =  next_state(curr_image,4)\n",
    "                next_state_tensor = tf.constant([[get_feature_value(nxt_state[1])]], dtype=tf.float32)\n",
    "                den_img = nxt_state[0]\n",
    "                reward = update_reward(den_img,curr_image)\n",
    "                cumulative_reward += reward\n",
    "                    \n",
    "            elif action == 5:\n",
    "                nxt_state =  next_state(curr_image,5)\n",
    "                next_state_tensor = tf.constant([[get_feature_value(nxt_state[1])]], dtype=tf.float32)\n",
    "                den_img = nxt_state[0]\n",
    "                reward = update_reward(den_img,curr_image)\n",
    "                cumulative_reward += reward\n",
    "                    \n",
    "\n",
    "            \n",
    "\n",
    "            # Compute the loss\n",
    "            loss = -tf.math.log(action_probs[0][action]) * reward\n",
    "\n",
    "            curr_image = den_img\n",
    "            end_ep_features, output_image = calculate_features(den_img)\n",
    "\n",
    "        print(end_ep_features)\n",
    "        plt.imshow(curr_image)\n",
    "        # plt.imshow(output_image)\n",
    "\n",
    "        # Compute gradients and update the policy network\n",
    "        grads = tape.gradient(loss, policy_net.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, policy_net.trainable_variables))\n",
    "\n",
    "        # Print episode information\n",
    "        print(f\"Episode {episode + 1}: State={state}, Action={image_enhancement_algorithms[action]}, Reward={reward},Cumulative={cumulative_reward}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_episodes = 1000\n",
    "discount_factor = 0.99\n",
    "\n",
    "# Train the agent\n",
    "train_REINFORCE(num_episodes, discount_factor,input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image = white_balance(input_img)\n",
    "image = Contrast_Down(image)\n",
    "image = Brightness_Up(image)\n",
    "image = Brightness_Up(image)\n",
    "features, image = calculate_features(image)\n",
    "print(features)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhance_underwater_image(img):\n",
    "    # img = cv2.imread(img_path)\n",
    "    # if img is None:\n",
    "    #    print(\"Error: Image not found.\")\n",
    "    #    return\n",
    "\n",
    "    # Convert image to RGB (OpenCV loads images in BGR)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # White balancing using the Gray World assumption\n",
    "    scale = img.mean(axis=(0, 1))\n",
    "    img_balanced = img * (scale.mean() / scale)\n",
    "\n",
    "    # Clip the values to [0, 255] and convert to uint8\n",
    "    img_balanced = np.clip(img_balanced, 0, 255).astype(np.uint8)\n",
    "\n",
    "    # Convert to LAB color space\n",
    "    lab = cv2.cvtColor(img_balanced, cv2.COLOR_RGB2LAB)\n",
    "    l, a, b = cv2.split(lab)\n",
    "\n",
    "    # Apply CLAHE to L-channel\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "\n",
    "    # Convert back to RGB color space\n",
    "    enhanced_img = cv2.cvtColor(limg, cv2.COLOR_LAB2RGB)\n",
    "\n",
    "    return enhanced_img\n",
    "image = enhance_underwater_image(input_img)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features, image = calculate_features(image)\n",
    "print(features)\n",
    "plt.imshow(image)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
