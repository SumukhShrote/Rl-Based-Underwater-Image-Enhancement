{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = cv2.imread('g:\\Images\\Original\\image_1721814223451221704.png')\n",
    "plt.imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_snr(image):\n",
    "    \n",
    "    if len(image.shape) == 3:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    mean_intensity = np.mean(image)\n",
    "    variance = np.var(image)\n",
    "    snr = 10 * np.log10(mean_intensity**2 / variance)\n",
    "\n",
    "    return snr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calculate_snr(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(image):\n",
    "    # Create ORB detector\n",
    "    orb = cv2.ORB_create()\n",
    "\n",
    "    # Detect keypoints using ORB\n",
    "    keypoints = orb.detect(image, None)\n",
    "\n",
    "    # Compute descriptors\n",
    "    keypoints, descriptors = orb.compute(image, keypoints)\n",
    "\n",
    "    # Draw the detected keypoints on the image\n",
    "    output_image = cv2.drawKeypoints(image, keypoints, None, color=(0, 255, 0), flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "\n",
    "    # Count the number of detected features\n",
    "    num_features = len(keypoints)\n",
    "    # print(f\"Number of ORB features detected: {num_features}\")\n",
    "\n",
    "    # Display the image with keypoints\n",
    "    plt.imshow(output_image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    return num_features\n",
    "\n",
    "calculate_features(input_img)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CLAHE(image):\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    else:\n",
    "        gray_image = image\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "\n",
    "    clahe_image = clahe.apply(gray_image)\n",
    "\n",
    "    if len(image.shape) == 3:\n",
    "        clahe_image = cv2.cvtColor(clahe_image, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return clahe_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_balance(image):\n",
    "   \n",
    "    lab_image = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "\n",
    "    l, a, b = cv2.split(lab_image)\n",
    "\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    cl = clahe.apply(l)\n",
    "\n",
    "    balanced_lab_image = cv2.merge((cl, a, b))\n",
    "\n",
    "    balanced_image = cv2.cvtColor(balanced_lab_image, cv2.COLOR_LAB2BGR)\n",
    "\n",
    "    return balanced_image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Contrast_Up(image):\n",
    "    \n",
    "    contrasted_image = cv2.convertScaleAbs(image, alpha=4.0, beta=0)\n",
    "    return contrasted_image\n",
    "\n",
    "def Contrast_Down(image):\n",
    "    \n",
    "    contrasted_image = cv2.convertScaleAbs(image, alpha=0.2, beta=0)\n",
    "    return contrasted_image\n",
    "\n",
    "def Brightness_Up(image):\n",
    "    \n",
    "    brightened_image = cv2.convertScaleAbs(image, alpha=1.0, beta=150)\n",
    "    return brightened_image\n",
    "\n",
    "def Brightness_Down(image):\n",
    "    \n",
    "    darkened_image = cv2.convertScaleAbs(image, alpha=1.0, beta=10)\n",
    "    return darkened_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self,image):\n",
    "        self.actions = ['WB','C_Up','C_Down','Bs_Up','B_Down','CLAHE']\n",
    "        self.states = ['F0','F1','F2','F3','F4','F5']\n",
    "        self.rewards = [-5,-1,1,2,3,4,5]\n",
    "        self.target_reached = 0\n",
    "        # self.step_limit = 6\n",
    "        self.memory = []\n",
    "        self.image = image\n",
    "        self.steps = 0\n",
    "        self.cumulative_reward = 0\n",
    "        self.learning_rate = 0.1\n",
    "        self.discount_factor = 0.9\n",
    "        self.exploration_prob = 0.6\n",
    "        self.num_episodes = 20\n",
    "        self.next_img = numpy.zeros_like(self.image)\n",
    "        self.action_taken = []\n",
    "\n",
    "        self.Q = np.zeros((len(self.states), len(self.actions)))\n",
    "\n",
    "    def check_state(self,image):\n",
    "        num_of_features = calculate_features(image)\n",
    "        if num_of_features < 0:\n",
    "            return 'F0'\n",
    "        elif num_of_features >= 0 and num_of_features < 100 :\n",
    "            return 'F1'\n",
    "        elif num_of_features <=200 and num_of_features > 100:\n",
    "            return 'F2'\n",
    "        elif num_of_features <=300 and num_of_features > 200:\n",
    "            return 'F3'\n",
    "        elif num_of_features <=400 and num_of_features > 300:\n",
    "            return 'F4'\n",
    "        elif  num_of_features > 400:\n",
    "            return 'F5'\n",
    "    \n",
    "    def get_feature_difference(self,ft1,ft2):\n",
    "        i1 = self.states.index(ft1)\n",
    "        i2 = self.states.index(ft2)\n",
    "        return (i2-i1) * 100\n",
    "\n",
    "    def update_reward(self,st1,st2):\n",
    "        print(st2,st1)\n",
    "        feature_difference = self.get_feature_difference(st2,st1)\n",
    "        \n",
    "        if feature_difference <0:\n",
    "            return -5\n",
    "        elif feature_difference == 0:\n",
    "            return -1\n",
    "        elif feature_difference <= 100 and feature_difference > 0:\n",
    "            return 1\n",
    "        elif feature_difference <= 200 and feature_difference > 100:\n",
    "            return 2\n",
    "        elif feature_difference <= 300 and feature_difference > 200:\n",
    "            return 3\n",
    "        elif feature_difference <= 400 and feature_difference > 300:\n",
    "            return 4\n",
    "        elif feature_difference > 400 :\n",
    "            return 5\n",
    "        \n",
    "    def update_memory(self,a,s,sd,r):\n",
    "        self.memory.append([a,s,sd,r])\n",
    "    \n",
    "    def perform_action(self,ind,img_inp):\n",
    "        self.steps += 1\n",
    "        if ind == 0:\n",
    "            denoised = white_balance(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 1:\n",
    "            denoised = Contrast_Up(img_inp)\n",
    "            return denoised      \n",
    "        elif ind == 2:\n",
    "            denoised = Contrast_Down(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 3:\n",
    "            denoised = Brightness_Up(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 4:\n",
    "            denoised = Brightness_Down(img_inp)\n",
    "            return denoised\n",
    "        elif ind == 5:\n",
    "            denoised = CLAHE(self.image)\n",
    "            return denoised\n",
    "    \n",
    "    def select_action(self,state):\n",
    "        if np.random.rand() < self.exploration_prob:\n",
    "            return np.random.choice(len(self.actions))  # Exploration: Choose a random action\n",
    "        else:\n",
    "            return np.argmax(self.Q[state, :])  # Exploitation: Choose the action with the highest Q-value\n",
    "    \n",
    "    def next_state(self,image ,action):\n",
    "        self.next_img = self.perform_action(action,image)\n",
    "        #next_img = self.perform_action(action,image)\n",
    "        return self.check_state(self.next_img), self.next_img\n",
    "        \n",
    "    def Q_train(self):\n",
    "    \n",
    "        curr_image = self.image\n",
    "\n",
    "        for episode in range(self.num_episodes):\n",
    "            init_state = self.check_state(self.image)\n",
    "            state = self.states.index(self.check_state(self.image))\n",
    "\n",
    "            for tries in range(100):\n",
    "                curr_state = self.check_state(curr_image)\n",
    "                action = self.select_action(state)\n",
    "                print(\"Action : \",action)\n",
    "                \n",
    "                # Perform the selected action and observe the next state and reward\n",
    "                if action == 0:\n",
    "                    next_state, _ = self.next_state(curr_image,0)\n",
    "                    print(curr_state,next_state)\n",
    "                    reward = self.update_reward(next_state,curr_state)\n",
    "                    self.update_memory(self.actions[action],curr_state,next_state,reward)\n",
    "                    self.cumulative_reward += reward\n",
    "                    self.action_taken.append(action)\n",
    "                elif action == 1:\n",
    "                    next_state, _ = self.next_state(curr_image,1)\n",
    "                    print(curr_state,next_state)\n",
    "                    reward = self.update_reward(next_state,curr_state)\n",
    "                    self.update_memory(self.actions[action],curr_state,next_state,reward)\n",
    "                    self.cumulative_reward += reward\n",
    "                    self.action_taken.append(action)\n",
    "                elif action == 2:\n",
    "                    next_state, _ = self.next_state(curr_image,2)\n",
    "                    print(curr_state,next_state)\n",
    "                    reward = self.update_reward(next_state,curr_state)\n",
    "                    self.update_memory(self.actions[action],curr_state,next_state,reward)\n",
    "                    self.cumulative_reward += reward\n",
    "                    self.action_taken.append(action)\n",
    "                elif action == 3:\n",
    "                    next_state, _ = self.next_state(curr_image,3) \n",
    "                    print(curr_state,next_state)\n",
    "                    reward = self.update_reward(next_state,curr_state)\n",
    "                    self.update_memory(self.actions[action],curr_state,next_state,reward)\n",
    "                    self.cumulative_reward += reward\n",
    "                    self.action_taken.append(action)\n",
    "                elif action == 4:\n",
    "                    next_state, _ = self.next_state(curr_image,4)\n",
    "                    print(curr_state,next_state)\n",
    "                    reward = self.update_reward(next_state,curr_state)\n",
    "                    self.update_memory(self.actions[action],curr_state,next_state,reward)\n",
    "                    self.cumulative_reward += reward\n",
    "                    self.action_taken.append(action)\n",
    "                elif action == 5:\n",
    "                    next_state, _ = self.next_state(curr_image,5)\n",
    "                    print(curr_state,next_state)\n",
    "                    reward = self.update_reward(next_state,curr_state)\n",
    "                    self.update_memory(self.actions[action],curr_state,next_state,reward)\n",
    "                    self.cumulative_reward += reward\n",
    "                    self.action_taken.append(action)\n",
    "                \n",
    "                print(self.memory)\n",
    "                print('cumulative_reward = ',self.cumulative_reward)\n",
    "                    \n",
    "        \n",
    "                # Update the Q-value using the Q-learning update rule \n",
    "                self.Q[self.states.index(curr_state), action] = self.Q[self.states.index(curr_state), action] + self.learning_rate * (reward + self.discount_factor * np.max(self.Q[self.states.index(next_state), :]) - self.Q[self.states.index(curr_state), action])\n",
    "                \n",
    "                state = self.states.index(next_state)  # Move to the next state\n",
    "                print(\"After action: \",curr_state,next_state)\n",
    "                print(\"Feature_Diff: \",self.get_feature_difference(curr_state,init_state))\n",
    "                #terminating condition\n",
    "                if self.get_feature_difference(curr_state,init_state) > 200 :\n",
    "                    #plt.imshow(curr_image)\n",
    "                    #plt.imshow(self.next_image)\n",
    "                    print(\"Target Reached\")\n",
    "                    \n",
    "                    break\n",
    "                if tries == 50:\n",
    "                    #plt.imshow(self.next_image)\n",
    "                    #plt.imshow(curr_image)\n",
    "                    print(\"Episode Force Stopped\")\n",
    "\n",
    "\n",
    "        # while self.target_reached == 0 and self.steps < self.step_limit:\n",
    "        #     print('Training')\n",
    "        #     init_features = len(fast12.detect(image, None))\n",
    "        #     select_action = random.randint(0,5)\n",
    "        #     denoised_img = self.perform_action(select_action,image)\n",
    "        #     image = denoised_img\n",
    "        #     temp_keypoints = fast12.detect(denoised_img, None)\n",
    "        #     num_features = len(temp_keypoints)\n",
    "        #     feature_diff = num_features - init_features\n",
    "        #     print(feature_diff)\n",
    "        #     self.update_memory(self.actions[select_action],self.check_state(init_features),self.check_state(num_features),self.update_reward)\n",
    "        #     self.cumulative_reward += self.update_reward(feature_diff)\n",
    "        #     print(self.cumulative_reward)\n",
    "        #     if feature_diff > 100:\n",
    "        #         print('Target_Reached',training_steps = self.steps,increase_in_features = feature_diff)\n",
    "        #         self.target_reached == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A1 = Agent(input_img)\n",
    "\n",
    "A1.Q_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(A1.action_taken)\n",
    "print(len(A1.action_taken))\n",
    "print(A1.steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(A1.next_img)\n",
    "cv2.imwrite(\"output_45.jpg\", A1.next_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards = []\n",
    "for i in A1.memory:\n",
    "    rewards.append(i[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Cumulative_sum(lists):   \n",
    "    cum_list = []   \n",
    "    length = len(lists)   \n",
    "    cum_list = [sum(lists[0:x:1]) for x in range(0, length+1)]   \n",
    "    return cum_list[1:]  \n",
    "   \n",
    "print (Cumulative_sum(rewards)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "\n",
    "# # Define Q-table to store Q-values for each state-action pair\n",
    "# num_states = 5\n",
    "# num_actions = 4  # Up, Down, Left, Right\n",
    "# Q = np.zeros((num_states, num_actions))\n",
    "\n",
    "# # Define parameters for the Q-learning algorithm\n",
    "# learning_rate = 0.1\n",
    "# discount_factor = 0.9\n",
    "# exploration_prob = 0.2\n",
    "# num_episodes = 1000\n",
    "\n",
    "# # Define a function to convert a state from the environment to an index\n",
    "# # def state_to_index(state):\n",
    "# #     return np.ravel_multi_index(np.where(env == state), env.shape)\n",
    "\n",
    "# # Define a function to select an action using epsilon-greedy policy\n",
    "# def select_action(state):\n",
    "#     if np.random.rand() < exploration_prob:\n",
    "#         return np.random.choice(num_actions)  # Exploration: Choose a random action\n",
    "#     else:\n",
    "#         return np.argmax(Q[state, :])  # Exploitation: Choose the action with the highest Q-value\n",
    "\n",
    "# # Q-learning training loop\n",
    "# for episode in range(num_episodes):\n",
    "#     state = self.states.index(check_state(self.image))  # Start from the initial state 'S'\n",
    "    \n",
    "#     while True:\n",
    "#         action = select_action(state)\n",
    "        \n",
    "#         # Perform the selected action and observe the next state and reward\n",
    "#         if action == 0:  # Up\n",
    "#             next_state = state_to_index(env[np.where(env == 'S')])  # Stay at 'S'\n",
    "#             reward = 0\n",
    "#         elif action == 1:  # Down\n",
    "#             next_state = state_to_index(env[np.where(env == 'H')])  # Hit an obstacle, stay there\n",
    "#             reward = 0\n",
    "#         elif action == 2:  # Left\n",
    "#             next_state = state_to_index(env[np.where(env == 'S')])  # Stay at 'S'\n",
    "#             reward = 0\n",
    "#         elif action == 3:  # Right\n",
    "#             next_state = state_to_index(env[np.where(env == ' ')] or env[np.where(env == 'G')])  # Move to an empty cell or the goal\n",
    "#             reward = 1 if env[np.where(env == 'G')] else 0  # +1 if reached the goal, else 0\n",
    "        \n",
    "#         # Update the Q-value using the Q-learning update rule\n",
    "#         Q[state, action] = Q[state, action] + learning_rate * (reward + discount_factor * np.max(Q[next_state, :]) - Q[state, action])\n",
    "        \n",
    "#         state = next_state  # Move to the next state\n",
    "        \n",
    "#         if env[np.where(env == 'G')]:  # Reached the goal\n",
    "#             break\n",
    "\n",
    "# # Now, the Q-table contains learned Q-values, and the agent can use them to make decisions\n",
    "# # You can use the Q-table to find the optimal policy or evaluate the agent's performance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
